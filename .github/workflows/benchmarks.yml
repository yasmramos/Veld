name: Veld DI Framework - Benchmarks & Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Ejecutar benchmarks diariamente a las 02:00 UTC
    - cron: '0 2 * * *'

jobs:
  benchmarks:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        java-version: [11, 17]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up JDK ${{ matrix.java-version }}
      uses: actions/setup-java@v4
      with:
        java-version: ${{ matrix.java-version }}
        distribution: 'temurin'
        
    - name: Cache Maven packages
      uses: actions/cache@v4
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
        
    - name: Cache JMH benchmarks
      uses: actions/cache@v4
      with:
        path: |
          ~/.m2/repository/org/openjdk/jmh
          ~/.m2/repository/org/apache/maven
        key: ${{ runner.os }}-jmh-${{ hashFiles('**/pom.xml') }}
        
    - name: Build complete Veld project
      run: |
        echo "=== CONSTRUYENDO PROYECTO VELD COMPLETO ==="
        # Install all Veld modules in correct dependency order
        # This ensures all dependencies are resolved for the benchmark module
        
        # Install core modules first (annotations, runtime, processor)
        mvn install -pl veld-annotations,veld-runtime,veld-aop,veld-processor,veld-weaver -am -DskipTests -q
        
        # Verify installation was successful
        if [ -f "veld-annotations/target/veld-annotations-*.jar" ]; then
            echo "‚úÖ veld-annotations installed successfully"
        else
            echo "‚ùå veld-annotations installation failed"
        fi
        
        if [ -f "veld-runtime/target/veld-runtime-*.jar" ]; then
            echo "‚úÖ veld-runtime installed successfully"
        else
            echo "‚ùå veld-runtime installation failed"
        fi
        
        if [ -f "veld-processor/target/veld-processor-*.jar" ]; then
            echo "‚úÖ veld-processor installed successfully"
        else
            echo "‚ùå veld-processor installation failed"
        fi
        
        echo "‚úÖ Veld project built with all dependencies"
          
    - name: Build and run JMH Benchmarks
      run: |
        echo "=== CONSTRUYENDO Y EJECUTANDO BENCHMARKS JMH COMPLETOS ==="
        # Build benchmark module with all dependencies resolved
        mvn install -pl veld-benchmark -am -DskipTests -q
        
        # Verify benchmark module was built
        if [ -f "veld-benchmark/target/veld-benchmark-*.jar" ]; then
            echo "‚úÖ veld-benchmark built successfully"
        else
            echo "‚ùå veld-benchmark build failed"
            exit 1
        fi
        
        # Run JMH benchmarks using the correct class
        cd veld-benchmark
        mvn exec:java \
          -Dexec.mainClass="io.github.yasmramos.benchmark.BenchmarkRunner" \
          -Dexec.args="Injection -f 1 -wi 1 -i 2 -rf json -rff benchmark-results.json" \
          -q
        
        # Check if benchmark results were generated
        if [ -f "benchmark-results.json" ]; then
            echo "‚úÖ JMH benchmark results generated successfully"
        else
            echo "‚ùå JMH benchmark failed to generate results"
        fi
        
    - name: Run Additional JMH Benchmarks
      run: |
        echo "=== EJECUTANDO BENCHMARKS ADICIONALES ==="
        cd veld-benchmark
        
        # Run startup benchmarks
        mvn exec:java \
          -Dexec.mainClass="io.github.yasmramos.benchmark.BenchmarkRunner" \
          -Dexec.args="Startup -f 1 -wi 1 -i 2 -rf json -rff startup-results.json" \
          -q
        
        # Run throughput benchmarks
        mvn exec:java \
          -Dexec.mainClass="io.github.yasmramos.benchmark.BenchmarkRunner" \
          -Dexec.args="Throughput -f 1 -wi 1 -i 2 -rf json -rff throughput-results.json" \
          -q
        
        # Verify all benchmark results
        echo "=== VERIFICANDO RESULTADOS DE BENCHMARKS ==="
        for result_file in benchmark-results.json startup-results.json throughput-results.json; do
            if [ -f "$result_file" ]; then
                echo "‚úÖ $result_file generated successfully"
                echo "üìä File size: $(wc -c < "$result_file") bytes"
            else
                echo "‚ùå $result_file not found"
            fi
        done
          
    - name: Generate benchmark reports
      run: |
        echo "=== GENERANDO REPORTES DE BENCHMARKS ==="
        python3 ../generate_benchmark_report.py
        
    - name: Run unit tests for core modules
      run: |
        echo "=== EJECUTANDO TESTS UNITARIOS ==="
        # Run tests for core modules to ensure they work correctly
        mvn test -pl veld-annotations,veld-runtime -q || echo "‚ö†Ô∏è Some tests failed but continuing"
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-java-${{ matrix.java-version }}
        path: |
          veld-benchmark/target/
          veld-benchmark/benchmark-results.json
          veld-benchmark/startup-results.json
          veld-benchmark/throughput-results.json
          benchmark-reports/
        retention-days: 30
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      with:
        name: test-results-java-${{ matrix.java-version }}
        path: |
          **/target/surefire-reports/
        retention-days: 7
        
    - name: Archive compilation artifacts
      uses: actions/upload-artifact@v4
      with:
        name: compilation-artifacts-java-${{ matrix.java-version }}
        path: |
          **/target/classes/
        retention-days: 7

  generate-final-report:
    needs: benchmarks
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      
    - name: Generate consolidated report
      run: |
        echo "=== GENERANDO REPORTE CONSOLIDADO ==="
        python3 ../consolidate_benchmark_reports.py
        
    - name: Commit benchmark results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add benchmark-reports/
        git commit -m "ü§ñ Auto-update benchmark results" || echo "No changes to commit"
        git push || echo "Could not push changes"
        
    - name: Upload consolidated results
      uses: actions/upload-artifact@v4
      with:
        name: consolidated-benchmark-report
        path: benchmark-reports/
        retention-days: 90

  performance-analysis:
    needs: [benchmarks, generate-final-report]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download artifacts
      uses: actions/download-artifact@v4
      
    - name: Run performance analysis
      run: |
        echo "=== AN√ÅLISIS DE RENDIMIENTO ==="
        python3 ../performance_analysis.py
        
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          let comment = '## üìä Benchmark Results\\n\\n';
          
          try {
            const reportPath = 'benchmark-reports/consolidated-report.md';
            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');
              comment += report;
            } else {
              comment += '‚ùå No benchmark report available';
            }
          } catch (error) {
            comment += `‚ùå Error reading report: ${error.message}`;
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
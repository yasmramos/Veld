name: Veld DI Framework - Benchmarks & Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Ejecutar benchmarks diariamente a las 02:00 UTC
    - cron: '0 2 * * *'

jobs:
  benchmarks:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        java-version: [11, 17]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up JDK ${{ matrix.java-version }}
      uses: actions/setup-java@v4
      with:
        java-version: ${{ matrix.java-version }}
        distribution: 'temurin'
        
    - name: Cache Maven packages
      uses: actions/cache@v4
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
        
    - name: Cache JMH benchmarks
      uses: actions/cache@v4
      with:
        path: |
          ~/.m2/repository/org/openjdk/jmh
          ~/.m2/repository/org/apache/maven
        key: ${{ runner.os }}-jmh-${{ hashFiles('**/pom.xml') }}
        
    - name: Build complete Veld project
      run: |
        echo "=== CONSTRUYENDO PROYECTO VELD COMPLETO ==="
        # Install Veld modules step by step to avoid dependency resolution issues
        
        # Step 1: Install parent POM first
        echo "üì¶ Instalando parent POM..."
        mvn install -N -DskipTests -q || {
            echo "‚ùå Parent POM installation failed"
            exit 1
        }
        
        # Step 2: Install core modules individually in dependency order
        echo "üì¶ Instalando veld-annotations..."
        mvn install -pl veld-annotations -am -DskipTests -q || {
            echo "‚ùå veld-annotations installation failed"
            exit 1
        }
        
        echo "üì¶ Instalando veld-runtime..."
        mvn install -pl veld-runtime -am -DskipTests -q || {
            echo "‚ùå veld-runtime installation failed"
            exit 1
        }
        
        echo "üì¶ Instalando veld-aop..."
        mvn install -pl veld-aop -am -DskipTests -q || {
            echo "‚ùå veld-aop installation failed"
            exit 1
        }
        
        echo "üì¶ Instalando veld-processor..."
        mvn install -pl veld-processor -am -DskipTests -q || {
            echo "‚ùå veld-processor installation failed"
            exit 1
        }
        
        echo "üì¶ Instalando veld-weaver..."
        mvn install -pl veld-weaver -am -DskipTests -q || {
            echo "‚ùå veld-weaver installation failed"
            exit 1
        }
        
        # Verify all installations were successful
        echo "üîç Verificando instalaciones..."
        local success=true
        
        for module in veld-annotations veld-runtime veld-aop veld-processor veld-weaver; do
            if [ -f "$module/target/$module-*.jar" ]; then
                echo "‚úÖ $module installed successfully"
            else
                echo "‚ùå $module installation failed"
                success=false
            fi
        done
        
        if [ "$success" = true ]; then
            echo "‚úÖ Veld project built with all dependencies"
        else
            echo "‚ùå Some modules failed to install"
            exit 1
        fi
          
    - name: Build and run JMH Benchmarks
      run: |
        echo "=== CONSTRUYENDO Y EJECUTANDO BENCHMARKS JMH COMPLETOS ==="
        
        # Build benchmark module with all dependencies resolved
        echo "üì¶ Construyendo veld-benchmark..."
        mvn install -pl veld-benchmark -am -DskipTests -q || {
            echo "‚ùå veld-benchmark build failed"
            echo "üîç Debugging build failure..."
            mvn install -pl veld-benchmark -am -DskipTests
            exit 1
        }
        
        # Verify benchmark module was built
        if [ -f "veld-benchmark/target/veld-benchmark-*.jar" ]; then
            echo "‚úÖ veld-benchmark built successfully"
        else
            echo "‚ùå veld-benchmark build failed - JAR not found"
            echo "üîç Checking target directory contents:"
            ls -la veld-benchmark/target/ || echo "Target directory not found"
            exit 1
        fi
        
        # Run JMH benchmarks using the correct class
        echo "üöÄ Ejecutando JMH benchmarks..."
        cd veld-benchmark
        mvn exec:java \
          -Dexec.mainClass="io.github.yasmramos.benchmark.BenchmarkRunner" \
          -Dexec.args="Injection -f 1 -wi 1 -i 2 -rf json -rff benchmark-results.json" \
          -q || {
            echo "‚ö†Ô∏è JMH benchmark execution failed, trying alternative approach..."
            # Alternative: run directly with java
            java -jar target/veld-benchmark.jar Injection -f 1 -wi 1 -i 2 -rf json -rff benchmark-results.json || {
                echo "‚ùå Both benchmark execution methods failed"
                exit 1
            }
        }
        
        # Check if benchmark results were generated
        if [ -f "benchmark-results.json" ]; then
            echo "‚úÖ JMH benchmark results generated successfully"
            echo "üìä Results file size: $(wc -c < benchmark-results.json) bytes"
        else
            echo "‚ùå JMH benchmark failed to generate results"
            echo "üîç Checking for any result files:"
            ls -la *.json 2>/dev/null || echo "No JSON files found"
        fi
        
    - name: Run Additional JMH Benchmarks
      run: |
        echo "=== EJECUTANDO BENCHMARKS ADICIONALES ==="
        cd veld-benchmark
        
        # Function to run benchmark with fallback
        run_benchmark() {
            local benchmark_name=$1
            local output_file=$2
            
            echo "üöÄ Ejecutando benchmark: $benchmark_name"
            
            # Try with Maven exec first
            if mvn exec:java \
                -Dexec.mainClass="io.github.yasmramos.benchmark.BenchmarkRunner" \
                -Dexec.args="$benchmark_name -f 1 -wi 1 -i 2 -rf json -rff $output_file" \
                -q; then
                echo "‚úÖ $benchmark_name benchmark completed successfully"
            else
                echo "‚ö†Ô∏è Maven exec failed for $benchmark_name, trying direct java..."
                # Fallback to direct java execution
                if java -jar target/veld-benchmark.jar "$benchmark_name" -f 1 -wi 1 -i 2 -rf json -rff "$output_file"; then
                    echo "‚úÖ $benchmark_name benchmark completed with direct java"
                else
                    echo "‚ùå $benchmark_name benchmark failed completely"
                    return 1
                fi
            fi
        }
        
        # Run startup benchmarks
        run_benchmark "Startup" "startup-results.json"
        
        # Run throughput benchmarks
        run_benchmark "Throughput" "throughput-results.json"
        
        # Verify all benchmark results
        echo "=== VERIFICANDO RESULTADOS DE BENCHMARKS ==="
        local success_count=0
        local total_count=0
        
        for result_file in benchmark-results.json startup-results.json throughput-results.json; do
            total_count=$((total_count + 1))
            if [ -f "$result_file" ]; then
                size=$(wc -c < "$result_file")
                if [ "$size" -gt 100 ]; then
                    echo "‚úÖ $result_file generated successfully ($size bytes)"
                    success_count=$((success_count + 1))
                else
                    echo "‚ö†Ô∏è $result_file generated but seems incomplete ($size bytes)"
                fi
            else
                echo "‚ùå $result_file not found"
            fi
        done
        
        echo "üìä Benchmark summary: $success_count/$total_count benchmarks completed successfully"
        
        if [ "$success_count" -ge 2 ]; then
            echo "‚úÖ Sufficient benchmark results generated"
        else
            echo "‚ö†Ô∏è Insufficient benchmark results, but continuing..."
        fi
          
    - name: Generate benchmark reports
      run: |
        echo "=== GENERANDO REPORTES DE BENCHMARKS ==="
        python3 ../generate_benchmark_report.py
        
    - name: Run unit tests for core modules
      run: |
        echo "=== EJECUTANDO TESTS UNITARIOS ==="
        # Run tests for core modules to ensure they work correctly
        mvn test -pl veld-annotations,veld-runtime -q || echo "‚ö†Ô∏è Some tests failed but continuing"
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-java-${{ matrix.java-version }}
        path: |
          veld-benchmark/target/
          veld-benchmark/benchmark-results.json
          veld-benchmark/startup-results.json
          veld-benchmark/throughput-results.json
          benchmark-reports/
        retention-days: 30
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      with:
        name: test-results-java-${{ matrix.java-version }}
        path: |
          **/target/surefire-reports/
        retention-days: 7
        
    - name: Archive compilation artifacts
      uses: actions/upload-artifact@v4
      with:
        name: compilation-artifacts-java-${{ matrix.java-version }}
        path: |
          **/target/classes/
        retention-days: 7

  generate-final-report:
    needs: benchmarks
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all artifacts
      uses: actions/download-artifact@v4
      
    - name: Generate consolidated report
      run: |
        echo "=== GENERANDO REPORTE CONSOLIDADO ==="
        python3 ../consolidate_benchmark_reports.py
        
    - name: Commit benchmark results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add benchmark-reports/
        git commit -m "ü§ñ Auto-update benchmark results" || echo "No changes to commit"
        git push || echo "Could not push changes"
        
    - name: Upload consolidated results
      uses: actions/upload-artifact@v4
      with:
        name: consolidated-benchmark-report
        path: benchmark-reports/
        retention-days: 90

  performance-analysis:
    needs: [benchmarks, generate-final-report]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download artifacts
      uses: actions/download-artifact@v4
      
    - name: Run performance analysis
      run: |
        echo "=== AN√ÅLISIS DE RENDIMIENTO ==="
        python3 ../performance_analysis.py
        
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          let comment = '## üìä Benchmark Results\\n\\n';
          
          try {
            const reportPath = 'benchmark-reports/consolidated-report.md';
            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');
              comment += report;
            } else {
              comment += '‚ùå No benchmark report available';
            }
          } catch (error) {
            comment += `‚ùå Error reading report: ${error.message}`;
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });